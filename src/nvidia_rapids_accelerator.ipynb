{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57fb4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ppaton\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "import os\n",
    "\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "print(home_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ce8000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397755e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/01 18:34:19 WARN Utils: Your hostname, ppaton-desktop resolves to a loopback address: 127.0.1.1; using 192.168.0.174 instead (on interface wlp3s0)\n",
      "25/06/01 18:34:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/01 18:34:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/01 18:34:19 WARN ResourceProfile: The executor resource config for resource: gpu was specified but no corresponding task resource request was specified.\n",
      "25/06/01 18:34:20 WARN RapidsPluginUtils: RAPIDS Accelerator 25.04.0 using cudf 25.04.0, private revision 74d87a45eb421fc84a84c6997b48c734ebe6556e\n",
      "25/06/01 18:34:20 WARN RapidsPluginUtils: Multiple cudf jars found in the classpath:\n",
      "revison: 6bc420631d12b0e7d87c5ede7710ac5b7154fd8d\n",
      "\tjar URL: jar:file:/home/ppaton/rapids_jars/rapids-4-spark_2.12-25.04.0.jar\n",
      "\tversion=25.04.0\n",
      "\tuser=root\n",
      "\trevision=6bc420631d12b0e7d87c5ede7710ac5b7154fd8d\n",
      "\tbranch=HEAD\n",
      "\tdate=2025-04-11T01:56:35Z\n",
      "\turl=https://github.com/rapidsai/cudf.git\n",
      "\tgpu_architectures=70;75;80;86;90\n",
      "\tjar URL: jar:file:/home/ppaton/rapids_jars/cudf-25.04.0-cuda12.jar\n",
      "\tversion=25.04.0\n",
      "\tuser=\n",
      "\trevision=6bc420631d12b0e7d87c5ede7710ac5b7154fd8d\n",
      "\tbranch=HEAD\n",
      "\tdate=2025-04-11T05:19:01Z\n",
      "Please make sure there is only one cudf jar in the classpath. If it is impossible to fix the classpath you can suppress the error by setting spark.rapids.sql.allowMultipleJars to ALWAYS, but this can cause unpredictable behavior as the plugin may pick up the wrong jar.\n",
      "25/06/01 18:34:20 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "25/06/01 18:34:20 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `ALL`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n",
      "25/06/01 18:34:27 WARN RapidsPluginUtils: Multiple cudf jars found in the classpath:\n",
      "revison: 6bc420631d12b0e7d87c5ede7710ac5b7154fd8d\n",
      "\tjar URL: jar:file:/home/ppaton/rapids_jars/rapids-4-spark_2.12-25.04.0.jar\n",
      "\tversion=25.04.0\n",
      "\tuser=root\n",
      "\trevision=6bc420631d12b0e7d87c5ede7710ac5b7154fd8d\n",
      "\tbranch=HEAD\n",
      "\tdate=2025-04-11T01:56:35Z\n",
      "\turl=https://github.com/rapidsai/cudf.git\n",
      "\tgpu_architectures=70;75;80;86;90\n",
      "\tjar URL: jar:file:/home/ppaton/rapids_jars/cudf-25.04.0-cuda12.jar\n",
      "\tversion=25.04.0\n",
      "\tuser=\n",
      "\trevision=6bc420631d12b0e7d87c5ede7710ac5b7154fd8d\n",
      "\tbranch=HEAD\n",
      "\tdate=2025-04-11T05:19:01Z\n",
      "Please make sure there is only one cudf jar in the classpath. If it is impossible to fix the classpath you can suppress the error by setting spark.rapids.sql.allowMultipleJars to ALWAYS, but this can cause unpredictable behavior as the plugin may pick up the wrong jar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor cluster at: http://ppaton-desktop.Home:4040\n"
     ]
    }
   ],
   "source": [
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .appName(\"GPU RAPIDS SQL Example\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.sql.explain\", \"ALL\") \\\n",
    "    .config(\"spark.executor.resource.gpu.amount\", \"1\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{home_dir}/rapids_jars/rapids-4-spark_2.12-25.04.0.jar:{home_dir}/rapids_jars/cudf-25.04.0-cuda12.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{home_dir}/rapids_jars/rapids-4-spark_2.12-25.04.0.jar:{home_dir}/rapids_jars/cudf-25.04.0-cuda12.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(f\"Monitor cluster at: {sc.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982c444",
   "metadata": {},
   "source": [
    "### Generate and process resilient distributed dataset (RDD) from an array on the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ed70fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd: RDD[int] = sc.parallelize(data)\n",
    "result = rdd.map(lambda x: x * 2).collect()\n",
    "print(result)  # Output: [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e288cb",
   "metadata": {},
   "source": [
    "### Generate and process a DataFrame from a .csv file on the driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362ce11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/01 18:34:52 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <FilterExec> will run on GPU\n",
      "    *Expression <GreaterThan> (length(trim(value#0, None)) > 0) will run on GPU\n",
      "      *Expression <Length> length(trim(value#0, None)) will run on GPU\n",
      "        *Expression <StringTrim> trim(value#0, None) will run on GPU\n",
      "    !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "25/06/01 18:34:54 WARN GpuOverrides:                                            \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <Invoke> value#0.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "    @Expression <AttributeReference> value#0 could run on GPU\n",
      "  !Expression <AttributeReference> obj#15 cannot run on GPU because expression AttributeReference obj#15 produces an unsupported type ObjectType(class java.lang.String)\n",
      "  !Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat\n",
      "\n",
      "[Stage 2:=====================================================>   (56 + 4) / 60]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: double (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: double (nullable = true)\n",
      " |-- _c4: double (nullable = true)\n",
      " |-- _c5: double (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      " |-- _c11: double (nullable = true)\n",
      " |-- _c12: double (nullable = true)\n",
      " |-- _c13: double (nullable = true)\n",
      " |-- _c14: double (nullable = true)\n",
      " |-- _c15: double (nullable = true)\n",
      " |-- _c16: double (nullable = true)\n",
      " |-- _c17: double (nullable = true)\n",
      " |-- _c18: double (nullable = true)\n",
      " |-- _c19: double (nullable = true)\n",
      " |-- _c20: double (nullable = true)\n",
      " |-- _c21: double (nullable = true)\n",
      " |-- _c22: double (nullable = true)\n",
      " |-- _c23: double (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: double (nullable = true)\n",
      " |-- _c26: double (nullable = true)\n",
      " |-- _c27: double (nullable = true)\n",
      " |-- _c28: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "higgs_data: DataFrame = spark.read.csv(\n",
    "    \"../data/HIGGS.csv\",\n",
    "    header=False,\n",
    "    inferSchema=True\n",
    ")\n",
    "higgs_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50899678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/01 18:35:44 WARN GpuOverrides: \n",
      "*Exec <HashAggregateExec> will run on GPU\n",
      "  *Expression <AggregateExpression> avg(_c0#17) will run on GPU\n",
      "    *Expression <Average> avg(_c0#17) will run on GPU\n",
      "  *Expression <AggregateExpression> stddev(_c0#17) will run on GPU\n",
      "    *Expression <StddevSamp> stddev(_c0#17) will run on GPU\n",
      "  *Expression <Alias> avg(_c0#17)#75 AS mean_c0#76 will run on GPU\n",
      "  *Expression <Alias> stddev(_c0#17)#115 AS stddev_c0#77 will run on GPU\n",
      "  *Exec <ShuffleExchangeExec> will run on GPU\n",
      "    *Partitioning <SinglePartition$> will run on GPU\n",
      "    *Exec <HashAggregateExec> will run on GPU\n",
      "      *Expression <AggregateExpression> partial_avg(_c0#17) will run on GPU\n",
      "        *Expression <Average> avg(_c0#17) will run on GPU\n",
      "      *Expression <AggregateExpression> partial_stddev(_c0#17) will run on GPU\n",
      "        *Expression <StddevSamp> stddev(_c0#17) will run on GPU\n",
      "      *Exec <FileSourceScanExec> will run on GPU\n",
      "\n",
      "25/06/01 18:35:44 WARN GpuOverrides: \n",
      "*Exec <HashAggregateExec> will run on GPU\n",
      "  *Expression <AggregateExpression> avg(_c0#17) will run on GPU\n",
      "    *Expression <Average> avg(_c0#17) will run on GPU\n",
      "  *Expression <AggregateExpression> stddev(_c0#17) will run on GPU\n",
      "    *Expression <StddevSamp> stddev(_c0#17) will run on GPU\n",
      "  *Expression <Alias> avg(_c0#17)#75 AS mean_c0#76 will run on GPU\n",
      "  *Expression <Alias> stddev(_c0#17)#115 AS stddev_c0#77 will run on GPU\n",
      "  *Exec <ShuffleExchangeExec> will run on GPU\n",
      "    *Partitioning <SinglePartition$> will run on GPU\n",
      "    *Exec <HashAggregateExec> will run on GPU\n",
      "      *Expression <AggregateExpression> partial_avg(_c0#17) will run on GPU\n",
      "        *Expression <Average> avg(_c0#17) will run on GPU\n",
      "      *Expression <AggregateExpression> partial_stddev(_c0#17) will run on GPU\n",
      "        *Expression <StddevSamp> stddev(_c0#17) will run on GPU\n",
      "      *Exec <FileSourceScanExec> will run on GPU\n",
      "\n",
      "25/06/01 18:35:44 WARN GpuOverrides: \n",
      "*Exec <HashAggregateExec> will run on GPU\n",
      "  *Expression <AggregateExpression> avg(_c0#17) will run on GPU\n",
      "    *Expression <Average> avg(_c0#17) will run on GPU\n",
      "  *Expression <AggregateExpression> stddev(_c0#17) will run on GPU\n",
      "    *Expression <StddevSamp> stddev(_c0#17) will run on GPU\n",
      "  *Expression <Alias> avg(_c0#17)#75 AS mean_c0#76 will run on GPU\n",
      "  *Expression <Alias> stddev(_c0#17)#115 AS stddev_c0#77 will run on GPU\n",
      "  *Exec <ShuffleExchangeExec> will run on GPU\n",
      "    *Partitioning <SinglePartition$> will run on GPU\n",
      "    *Exec <HashAggregateExec> will run on GPU\n",
      "      *Expression <AggregateExpression> partial_avg(_c0#17) will run on GPU\n",
      "        *Expression <Average> avg(_c0#17) will run on GPU\n",
      "      *Expression <AggregateExpression> partial_stddev(_c0#17) will run on GPU\n",
      "        *Expression <StddevSamp> stddev(_c0#17) will run on GPU\n",
      "      *Exec <FileSourceScanExec> will run on GPU\n",
      "\n",
      "25/06/01 18:35:44 WARN GpuOverrides: \n",
      "*Exec <ShuffleExchangeExec> will run on GPU\n",
      "  *Partitioning <SinglePartition$> will run on GPU\n",
      "  *Exec <HashAggregateExec> will run on GPU\n",
      "    *Expression <AggregateExpression> partial_avg(_c0#17) will run on GPU\n",
      "      *Expression <Average> avg(_c0#17) will run on GPU\n",
      "    *Expression <AggregateExpression> partial_stddev(_c0#17) will run on GPU\n",
      "      *Expression <StddevSamp> stddev(_c0#17) will run on GPU\n",
      "    *Exec <FileSourceScanExec> will run on GPU\n",
      "\n",
      "25/06/01 18:35:48 WARN GpuOverrides: ========================>    (55 + 5) / 60]\n",
      "*Exec <HashAggregateExec> will run on GPU\n",
      "  *Expression <AggregateExpression> avg(_c0#17) will run on GPU\n",
      "    *Expression <Average> avg(_c0#17) will run on GPU\n",
      "  *Expression <AggregateExpression> stddev(_c0#17) will run on GPU\n",
      "    *Expression <StddevSamp> stddev(_c0#17) will run on GPU\n",
      "  *Expression <Alias> avg(_c0#17)#75 AS mean_c0#76 will run on GPU\n",
      "  *Expression <Alias> stddev(_c0#17)#115 AS stddev_c0#77 will run on GPU\n",
      "\n",
      "25/06/01 18:35:48 WARN GpuOverrides: \n",
      "*Exec <HashAggregateExec> will run on GPU\n",
      "  *Expression <AggregateExpression> avg(_c0#17) will run on GPU\n",
      "    *Expression <Average> avg(_c0#17) will run on GPU\n",
      "  *Expression <AggregateExpression> stddev(_c0#17) will run on GPU\n",
      "    *Expression <StddevSamp> stddev(_c0#17) will run on GPU\n",
      "  *Expression <Alias> avg(_c0#17)#75 AS mean_c0#76 will run on GPU\n",
      "  *Expression <Alias> stddev(_c0#17)#115 AS stddev_c0#77 will run on GPU\n",
      "\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_c0</th>\n",
       "      <th>stddev_c0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.52992</td>\n",
       "      <td>0.499104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_c0  stddev_c0\n",
       "0  0.52992   0.499104"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    higgs_data\n",
    "    .agg(\n",
    "        F.mean(F.col(\"_c0\")).alias(\"mean_c0\"),\n",
    "        F.stddev(F.col(\"_c0\")).alias(\"stddev_c0\"),\n",
    "    )\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09719201",
   "metadata": {},
   "source": [
    "### Free cluster resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd84c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
